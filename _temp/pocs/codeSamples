Got it! You want a Spring Kafka configuration class where:

1. POD ID is retrieved dynamically from a DB sequence service instead of an environment variable.


2. Custom Partition Assignor assigns one partition per POD dynamically.


3. Kafka listener processes only one message at a time, ensuring each POD handles messages sequentially.




---

Step 1: Define a Service to Fetch POD ID from the DB

You'll need a service class that retrieves a unique POD ID from a DB sequence.

PodIdService.java

import org.springframework.stereotype.Service;
import java.util.concurrent.atomic.AtomicInteger;

@Service
public class PodIdService {
    private final AtomicInteger podIdSequence = new AtomicInteger(0); // Simulating DB sequence

    public int getPodId() {
        return podIdSequence.incrementAndGet(); // Fetching ID from DB sequence (Simulated)
    }
}

> Note: Replace this logic with an actual DB call (e.g., JPARepository or JDBC query) in real-world applications.




---

Step 2: Configure Kafka Consumer for Upload

This configuration:

Fetches POD ID dynamically from PodIdService

Ensures one message is processed at a time (max.poll.records=1)

Assigns partitions using CustomPartitionAssignor


KafkaConsumerConfig.java

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class KafkaConsumerConfig {

    private final PodIdService podIdService;

    public KafkaConsumerConfig(PodIdService podIdService) {
        this.podIdService = podIdService;
    }

    @Value("${spring.kafka.bootstrap-servers}")
    private String bootstrapServers;

    @Bean
    public ConsumerFactory<String, String> uploadConsumerFactory() {
        Map<String, Object> config = new HashMap<>();
        int podId = podIdService.getPodId(); // Get POD ID dynamically

        config.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        config.put(ConsumerConfig.GROUP_ID_CONFIG, "upload-consumer-group");
        config.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        config.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        config.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, "1"); // Process only one message at a time
        config.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true"); // Auto-commit after processing
        config.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, CustomPartitionAssignor.class.getName());
        config.put("custom.id", podId);  // Pass dynamically fetched POD ID

        return new DefaultKafkaConsumerFactory<>(config);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> uploadKafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(uploadConsumerFactory());
        factory.setBatchListener(false); // Ensure single-message processing
        return factory;
    }
}


---

Step 3: Implement Custom Partition Assignor

This class:

Retrieves the dynamically assigned POD ID

Assigns one partition per POD


CustomPartitionAssignor.java

import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;
import org.apache.kafka.clients.consumer.GroupSubscription;
import org.apache.kafka.clients.consumer.Subscription;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.clients.consumer.internals.Assignment;

import java.util.*;

public class CustomPartitionAssignor implements ConsumerPartitionAssignor {
    private int podId = 1; // Default POD ID

    @Override
    public void configure(Map<String, ?> configs) {
        if (configs.containsKey("custom.id")) {
            podId = Integer.parseInt(configs.get("custom.id").toString());
        }
    }

    @Override
    public GroupAssignment assign(Cluster metadata, GroupSubscription groupSubscription) {
        Map<String, Assignment> assignmentMap = new HashMap<>();

        for (Map.Entry<String, Subscription> entry : groupSubscription.groupSubscription().entrySet()) {
            String consumerId = entry.getKey();
            Subscription subscription = entry.getValue();

            List<TopicPartition> assignedPartitions = new ArrayList<>();
            for (String topic : subscription.topics()) {
                int numPartitions = metadata.partitionCountForTopic(topic);
                if (numPartitions > 0) {
                    int partition = (podId - 1) % numPartitions;  // Assign partition dynamically
                    assignedPartitions.add(new TopicPartition(topic, partition));
                }
            }

            assignmentMap.put(consumerId, new Assignment(assignedPartitions));
        }

        return new GroupAssignment(assignmentMap);
    }

    @Override
    public String name() {
        return "CustomPartitionAssignor";
    }
}


---

Step 4: Implement Kafka Listener for Upload

This listener:

Listens to upload-topic

Processes only one message at a time


UploadConsumer.java

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

@Component
public class UploadConsumer {

    @KafkaListener(topics = "upload-topic", containerFactory = "uploadKafkaListenerContainerFactory")
    public void listenUpload(String message) {
        System.out.println("Processing Upload Message: " + message);

        // Simulating message processing
        try {
            Thread.sleep(2000); // Simulate processing delay
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }

        System.out.println("Finished Processing: " + message);
    }
}


---

How This Works

1. POD ID Retrieval

PodIdService fetches a unique ID from the DB sequence.

The ID is passed as "custom.id" in the Kafka consumer configuration.



2. Custom Partition Assignment

CustomPartitionAssignor assigns one partition per POD, ensuring messages go to the right partition.



3. Single Message Processing

max.poll.records=1 ensures the consumer fetches only one message at a time.

The Kafka listener processes the message before fetching another.





---

Key Benefits

✅ Dynamic POD ID Assignment: Retrieved from DB sequence service instead of hardcoded config.
✅ One Message at a Time: Ensures proper load distribution and prevents race conditions.
✅ Custom Partition Assignment: Each POD gets one dedicated partition, avoiding conflicts.
✅ Scalable for Kubernetes: Works seamlessly when running multiple PODs.


---

Next Steps

If running in Kubernetes, ensure PodIdService assigns unique IDs to each POD.

Replace Thread.sleep() with actual business logic inside UploadConsumer.


Let me know if you need any modifications!

