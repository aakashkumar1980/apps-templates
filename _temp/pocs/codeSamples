If you'd like to set up two Kafka consumers without running them through a main class, and instead handle this with proper Spring Boot components, you can create two separate consumers using Spring Kafka's @KafkaListener annotations or by configuring two KafkaListenerContainerFactory beans. Here's how you can implement two independent consumers for upload and download within a Spring Boot application.

1. Set up Spring Boot with Kafka:

Ensure you have the necessary dependencies in your pom.xml:

<dependency>
    <groupId>org.springframework.kafka</groupId>
    <artifactId>spring-kafka</artifactId>
</dependency>
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter</artifactId>
</dependency>

2. Configuration Class for Kafka Consumers

We will create two consumers, one for uploading and one for downloading, using a custom PartitionAssignor. We will also configure two consumer groups and set dynamic partition assignments based on pod-specific IDs.

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.apache.kafka.clients.consumer.Consumer;
import org.springframework.kafka.annotation.EnableKafka;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;
import org.springframework.kafka.listener.MessageListenerContainer;
import org.springframework.kafka.listener.config.DefaultKafkaListenerContainerFactory;
import org.springframework.kafka.listener.MessageListener;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.apache.kafka.common.serialization.Deserializer;
import org.springframework.kafka.listener.ConcurrentMessageListenerContainer;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.listener.MessageListenerContainer;
import org.springframework.kafka.annotation.EnableKafka;

import org.apache.kafka.clients.consumer.KafkaConsumer;

@Configuration
@EnableKafka
public class KafkaConsumerConfig {

    // Bean to provide upload consumer configuration
    @Bean
    public Consumer<String, String> uploadConsumer() {
        Properties properties = new Properties();
        properties.put("bootstrap.servers", "localhost:9092");
        properties.put("group.id", "upload-consumer-group");
        properties.put("enable.auto.commit", "false");
        properties.put("key.deserializer", StringDeserializer.class);
        properties.put("value.deserializer", StringDeserializer.class);
        properties.put("partition.assignment.strategy", CustomPartitionAssignor.class.getName());
        
        // Dynamic pod ID for upload consumer
        properties.put("custom.id", 1); // Replace with your dynamic value
        
        return new KafkaConsumer<>(properties);
    }

    // Bean to provide download consumer configuration
    @Bean
    public Consumer<String, String> downloadConsumer() {
        Properties properties = new Properties();
        properties.put("bootstrap.servers", "localhost:9092");
        properties.put("group.id", "download-consumer-group");
        properties.put("enable.auto.commit", "false");
        properties.put("key.deserializer", StringDeserializer.class);
        properties.put("value.deserializer", StringDeserializer.class);
        properties.put("partition.assignment.strategy", CustomPartitionAssignor.class.getName());
        
        // Dynamic pod ID for download consumer
        properties.put("custom.id", 2); // Replace with your dynamic value
        
        return new KafkaConsumer<>(properties);
    }

    // Listener container for Upload consumer (you can customize the listeners)
    @Bean
    public MessageListenerContainer uploadListenerContainer() {
        DefaultKafkaListenerContainerFactory<String, String> factory = new DefaultKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(new DefaultKafkaConsumerFactory<>(uploadConsumer()));
        factory.setConcurrency(1);

        ConcurrentMessageListenerContainer<String, String> container = 
            new ConcurrentMessageListenerContainer<>(factory, new KafkaListenerContainer(String, String));
        
        return container;
    }

    // Listener container for Download consumer
    @Bean
    public MessageListenerContainer downloadListenerContainer() {
        DefaultKafkaListenerContainerFactory<String, String> factory = new DefaultKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(new DefaultKafkaConsumerFactory<>(downloadConsumer()));
        factory.setConcurrency(1);

        ConcurrentMessageListenerContainer<String, String> container = 
            new ConcurrentMessageListenerContainer<>(factory, new KafkaListenerContainer(String, String));
        
        return container;
    }
}

3. Kafka Listeners for Upload and Download:

Now, use @KafkaListener to consume messages. The @KafkaListener annotation makes it easy to create listeners that consume messages from topics.

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class UploadConsumerService {

    // This method will consume messages from the "upload-topic"
    @KafkaListener(topics = "upload-topic", groupId = "upload-consumer-group")
    public void listenUploadTopic(String message) {
        // Handle the uploaded message
        System.out.println("Upload Consumer received message: " + message);
    }
}

@Service
public class DownloadConsumerService {

    // This method will consume messages from the "download-topic"
    @KafkaListener(topics = "download-topic", groupId = "download-consumer-group")
    public void listenDownloadTopic(String message) {
        // Handle the downloaded message
        System.out.println("Download Consumer received message: " + message);
    }
}

4. Custom Partition Assignor:

The CustomPartitionAssignor is used to control partition assignments dynamically. Hereâ€™s how it integrates into the Spring Kafka configuration.

import org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;
import org.apache.kafka.clients.consumer.GroupSubscription;
import org.apache.kafka.clients.consumer.Subscription;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.clients.consumer.internals.Assignment;

import java.util.*;

public class CustomPartitionAssignor implements ConsumerPartitionAssignor {

    private final Integer id;

    // Constructor to initialize with the dynamic pod ID
    public CustomPartitionAssignor(Integer id) {
        this.id = id;
    }

    @Override
    public GroupAssignment assign(Cluster metadata, GroupSubscription groupSubscription) {
        Map<String, Assignment> assignmentMap = new HashMap<>();

        for (Map.Entry<String, Subscription> entry : groupSubscription.groupSubscription().entrySet()) {
            String consumerId = entry.getKey();  // Consumer client ID
            Subscription subscription = entry.getValue();

            List<TopicPartition> assignedPartitions = new ArrayList<>();

            // Loop through each subscribed topic
            for (String topic : subscription.topics()) {
                int numPartitions = metadata.partitionCountForTopic(topic);
                if (numPartitions > 0) {
                    int partition = (id - 1) % numPartitions;  // Assign based on pod-specific ID
                    assignedPartitions.add(new TopicPartition(topic, partition));
                }
            }

            // Create an Assignment object and map it to the consumer
            assignmentMap.put(consumerId, new Assignment(assignedPartitions));
        }

        return new GroupAssignment(assignmentMap);
    }

    @Override
    public String name() {
        return "CustomPartitionAssignor";
    }
}

Explanation:

1. Consumer Beans:

We define two Kafka consumer beans: uploadConsumer and downloadConsumer. Each one has its own configuration with different topics, group IDs, and dynamic pod IDs.



2. Listener Containers:

uploadListenerContainer and downloadListenerContainer are configured to manage the Kafka listeners for the upload and download consumers. These containers manage the lifecycle of consumer threads and handle message consumption.



3. Listeners (@KafkaListener):

The @KafkaListener annotations are used to create listeners for consuming messages from the respective topics (upload-topic and download-topic).



4. Custom Partition Assignor:

The custom assignor (CustomPartitionAssignor) is responsible for dynamically assigning partitions based on the pod-specific id. This ensures that each consumer processes a specific partition based on the pod it represents.




Run Your Application:

Now, you can run your Spring Boot application, and the two consumers (UploadConsumerService and DownloadConsumerService) will independently consume messages from their respective topics (upload-topic and download-topic), while the custom partition assignment logic ensures that each pod processes its assigned partition.

Let me know if you need any further details!

